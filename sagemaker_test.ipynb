{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9a1899a-304f-4c88-a25e-9889163470fc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version is good\n",
      "Region = ap-southeast-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sys \n",
    "import IPython \n",
    "\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Installing previous SageMaker Version and restarting the kernel\")\n",
    "    !{sys.executable} -m pip install sagemaker==1.72.0\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "else:\n",
    "    print(\"Version is good\")\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client('sagemaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a77fc02-a307-4f73-b2c1-0bc0a70848b9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d4ec949-dc4e-4964-9163-aa61b6143915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.11/site-packages (0.1.45)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.11/site-packages (from sagemaker-experiments) (1.36.3)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.3 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.36.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.11.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.3->boto3>=1.16.27->sagemaker-experiments) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.3->boto3>=1.16.27->sagemaker-experiments) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.3->boto3>=1.16.27->sagemaker-experiments) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker-experiments \n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65b934c9-789e-40ef-8120-1bf48b009db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawbucket = sess.default_bucket() # Default bucket in S3\n",
    "\n",
    "prefix = 'sagemaker-modelmonitor'\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + 'test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "280695d0-2e85-4f67-9dc8-e74b8d7a126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 09:44:48--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "connected. to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... \n",
      "WARNING: cannot verify archive.ics.uci.edu's certificate, issued by ‘CN=E6,O=Let's Encrypt,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: unspecified\n",
      "Saving to: ‘default of credit card clients.xls.5’\n",
      "\n",
      "default of credit c     [             <=>    ]   5.28M  1.47MB/s    in 3.9s    \n",
      "\n",
      "2025-03-11 09:44:52 (1.35 MB/s) - ‘default of credit card clients.xls.5’ saved [5539328]\n",
      "\n",
      "Requirement already satisfied: xlrd in /opt/conda/lib/python3.11/site-packages (2.0.1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...          0          0          0         0       689         0   \n",
       "1      0  ...       3272       3455       3261         0      1000      1000   \n",
       "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls --no-check-certificate\n",
    "!pip install xlrd\n",
    "data = pd.read_excel('default of credit card clients.xls', header=1)\n",
    "data = data.drop(columns = ['ID'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a1a0a353-196e-4ec2-a0e6-2c4e63499412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0      1      20000    2          2         1   24      2      2     -1   \n",
       "1      1     120000    2          2         2   26     -1      2      0   \n",
       "2      0      90000    2          2         2   34      0      0      0   \n",
       "3      0      50000    2          2         1   37      0      0      0   \n",
       "4      0      50000    1          2         1   57     -1      0     -1   \n",
       "\n",
       "   PAY_4  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0     -1  ...        689          0          0          0         0       689   \n",
       "1      0  ...       2682       3272       3455       3261         0      1000   \n",
       "2      0  ...      13559      14331      14948      15549      1518      1500   \n",
       "3      0  ...      49291      28314      28959      29547      2000      2019   \n",
       "4      0  ...      35835      20940      19146      19131      2000     36681   \n",
       "\n",
       "   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "0         0         0         0         0  \n",
       "1      1000      1000         0      2000  \n",
       "2      1000      1000      1000      5000  \n",
       "3      1200      1100      1069      1000  \n",
       "4     10000      9000       689       679  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1, join='outer')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17c6385c-25d5-4d23-a1c3-904e3c44cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/data\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass\n",
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix) # Upload to S3\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d952d89-9fe3-4266-8970-23910d56375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data using Amazon SageMaker Processing\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                    role=role, instance_type='ml.c4.xlarge', instance_count=1) # role= get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f5a87e0-c63e-4e36-ac29-95600fe38031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--random-split', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    COLS = df.columns\n",
    "    newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state=args.random_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n",
    "                                                        test_size=split_ratio, random_state=random_state)\n",
    "    \n",
    "    preprocess = make_column_transformer(\n",
    "        (['PAY_AMT1'], StandardScaler()),\n",
    "        (['BILL_AMT1'], MinMaxScaler()),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "    \n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1, join='outer')\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1, join='outer')\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "                 \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70871914-a933-45ab-92c0-990cd5c4bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "# copy to s3 and run\n",
    "\n",
    "codeprefix = prefix + '/code'\n",
    "codeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bdb9c36e-0c9f-487f-b505-038352134039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data location = sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/train_data\n",
      "Test data location = sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitortest_data\n"
     ]
    }
   ],
   "source": [
    "# Where to store training and test data after SageMaker Processing job completes.\n",
    "\n",
    "train_data_location = rawbucket + '/' + traindataprefix\n",
    "test_data_location = rawbucket + '/' + testdataprefix\n",
    "\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8948acb8-5959-417b-ae26-b4a70b73d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2025-03-11-12-02-11-619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2025-03-11-12-02-11-619\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/data', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/train_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitortest_data', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'train_data_headers', 'S3Output': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/train_headers', 'LocalPath': '/opt/ml/processing/train_headers', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...........\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(random_split=0, train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/rawdata.csv\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (24000, 23)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (6000, 23)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_data.csv\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\u001b[34mSave training data with headers to /opt/ml/processing/train_headers/train_data_with_headers.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/test/test_data.csv\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy and paste the following code to start processing job.\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code=codeupload,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_data_location,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                               destination='s3://' + train_data_location),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination=\"s3://\"+test_data_location),\n",
    "                               ProcessingOutput(output_name='train_data_headers',\n",
    "                                                source='/opt/ml/processing/train_headers',\n",
    "                                               destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a7910a3-ec87-43eb-8114-bf6b69ed3572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7fbd47b8fc50>,experiment_name='Build-train-deploy-1741694723',description='Predict credit card default from payments data',tags=None,experiment_arn='arn:aws:sagemaker:ap-southeast-1:438465157691:experiment/Build-train-deploy-1741694723',response_metadata={'RequestId': '61c8a7df-a088-4f7f-a933-788da5dc565d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '61c8a7df-a088-4f7f-a933-788da5dc565d', 'content-type': 'application/x-amz-json-1.1', 'content-length': '106', 'date': 'Tue, 11 Mar 2025 12:05:23 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# Create an Amazon SageMaker Experiment\n",
    "\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f'Build-train-deploy-{int(time.time())}',\n",
    "    description=\"Predict credit card default from payments data\",\n",
    "    sagemaker_boto_client=sm, # sm = boto3.Session().client('sagemaker')\n",
    ")\n",
    "print(cc_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9957763-a137-49b4-b082-b6e2146cabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({        \"train_test_split_ratio\": 0.2,\n",
    "                                    \"random_state\":0 })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c843902-8a6a-4319-8334-c59d014e8dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker.estimator:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: cc-training-job-1741689296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-11 10:34:57 Starting - Starting the training job...\n",
      "..25-03-11 10:35:19 Starting - Preparing the instances for training.\n",
      "..25-03-11 10:35:39 Downloading - Downloading input data.\n",
      ".....03-11 10:36:05 Downloading - Downloading the training image.\n",
      "2025-03-11 10:37:26 Training - Training image download completed. Training in progress.\n",
      "\u001b[34m[2025-03-11 10:37:18.428 ip-10-0-172-255.ap-southeast-1.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 24000 rows\u001b[0m\n",
      "\u001b[34m[10:37:18] 24000x23 matrix with 552000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.17854\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.17746\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.17704\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.17717\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.17621\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.17608\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.17504\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.17467\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.17513\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.17475\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.17408\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.17354\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.17363\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.17313\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.17325\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.17296\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.17292\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.17317\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.17250\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.17200\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.17175\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.17142\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.17154\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.17171\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.17112\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.17033\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.17046\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.17021\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.16996\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.16971\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.16929\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.16917\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.16908\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.16904\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.16900\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.16867\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.16846\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.16825\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.16817\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.16817\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.16800\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.16804\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.16804\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.16792\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.16737\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.16717\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.16654\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.16667\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.16642\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.16629\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.16542\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.16517\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.16529\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.16529\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.16521\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.16500\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.16500\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.16487\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.16408\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.16400\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.16375\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.16367\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.16333\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.16337\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.16317\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.16296\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.16287\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.16308\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.16300\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.16300\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.16271\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.16246\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.16237\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.16221\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.16171\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.16179\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.16096\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.16079\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.16067\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.16071\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.16042\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.16017\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.16000\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.16029\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.16012\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.16025\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.15975\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.15983\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.15967\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.15937\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.15887\u001b[0m\n",
      "\n",
      "2025-03-11 10:37:34 Completed - Training job completed\n",
      "Training seconds: 115\n",
      "Billable seconds: 115\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://' + train_data_location, content_type='csv')\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"cc-default-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "            experiment_name=cc_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm\n",
    "    )\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    train_max_run=86400,\n",
    "                                    output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                                    sagemaker_session=sess) # set to true for distributed training\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        verbosity=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(inputs = {'train':s3_input_train},\n",
    "       job_name=cc_training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4115f0c4-4277-4b6d-8623-551d11979515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitortest_data/test_data.csv to ./test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Deploy the model for offline inference\n",
    "\n",
    "test_data_path = 's3://' + test_data_location + '/test_data.csv'\n",
    "! aws s3 cp $test_data_path ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c3b3a10-0e21-4bbe-9550-50595777e01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>0.201175</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17399.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>18453.0</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>19288.0</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>0.199594</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19347.0</td>\n",
       "      <td>18600.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.284364</td>\n",
       "      <td>0.185736</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.040569</td>\n",
       "      <td>0.289360</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99998.0</td>\n",
       "      <td>16138.0</td>\n",
       "      <td>17758.0</td>\n",
       "      <td>18774.0</td>\n",
       "      <td>20272.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>0.186502</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>6469.0</td>\n",
       "      <td>5138.0</td>\n",
       "      <td>7810.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>7833.0</td>\n",
       "      <td>7130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3    4    5    6     7    8    9  ...  \\\n",
       "0  0 -0.341476  0.201175   20000.0  1.0  1.0  2.0  33.0  1.0  2.0  ...   \n",
       "1  0 -0.136859  0.199594   20000.0  2.0  2.0  2.0  35.0  0.0  0.0  ...   \n",
       "2  0 -0.284364  0.185736  230000.0  2.0  1.0  1.0  44.0  1.0 -1.0  ...   \n",
       "3  0 -0.040569  0.289360  100000.0  1.0  2.0  1.0  42.0  0.0  0.0  ...   \n",
       "4  0  0.079132  0.186502  150000.0  1.0  1.0  2.0  29.0 -2.0 -2.0  ...   \n",
       "\n",
       "        14       15       16       17       18      19      20      21  \\\n",
       "0  17399.0  19057.0  18453.0  19755.0  19288.0  2260.0     0.0  1600.0   \n",
       "1  19347.0  18600.0  19000.0  19000.0  20000.0     0.0  1000.0     0.0   \n",
       "2    949.0   2864.0    933.0      0.0      0.0  2873.0   933.0     0.0   \n",
       "3  99998.0  16138.0  17758.0  18774.0  20272.0  2000.0  2000.0  2000.0   \n",
       "4   6917.0    831.0   6469.0   5138.0   7810.0   833.0  6488.0  5153.0   \n",
       "\n",
       "       22      23  \n",
       "0     0.0   644.0  \n",
       "1  1000.0     0.0  \n",
       "2     0.0     0.0  \n",
       "3  2000.0  2000.0  \n",
       "4  7833.0  7130.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full = pd.read_csv('test_data.csv', names = [str(x) for x in range(len(data.columns))])\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fab7704f-8c3b-4fea-8122-aa416ca14d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = test_full['0'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2da0b6da-9b4e-44f7-a424-bf7def78eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating model with name: cc-training-job-1741689296\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-03-11-10-42-33-819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m[2025-03-11:10:46:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-11:10:46:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-11:10:46:44:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2025-03-11 10:46:45 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2025-03-11:10:46:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Mar/2025:10:46:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-03-11:10:46:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Mar/2025:10:46:49 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-03-11:10:46:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Mar/2025:10:46:50 +0000] \"POST /invocations HTTP/1.1\" 200 118175 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[32m2025-03-11T10:46:49.929:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "CPU times: user 609 ms, sys: 46.2 ms, total: 655 ms\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_path, split_type='Line', input_filter='$[1:]', content_type='text/csv')\n",
    "sm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec40a183-9c5c-4227-a681-5ae9b8830534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4460</td>\n",
       "      <td>815</td>\n",
       "      <td>5275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>243</td>\n",
       "      <td>482</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4703</td>\n",
       "      <td>1297</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "Actual                     \n",
       "0.0        4460   815  5275\n",
       "1.0         243   482   725\n",
       "All        4703  1297  6000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model metrics\n",
    "\n",
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')\n",
    "output = get_csv_output_from_s3(sm_transformer.output_path, 'test_data.csv.out')\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)\n",
    "output_df['Predicted']=np.round(output_df.values)\n",
    "output_df['Label'] = label\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57e1779f-3671-4a07-9624-d07905f6b9c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline Accuracy = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39munique(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]))))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy Score = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy_score(label, output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "# print(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\n",
    "# print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['Predicted'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd4d65a8-6e03-4436-b2c0-19663221d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: arn:aws:sagemaker:ap-southeast-1:438465157691:model/cc-training-job-1741689296\n"
     ]
    }
   ],
   "source": [
    "# Deploy the model as an endpoint and set up data capture\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "latest_training_job = sm_client.list_training_jobs(MaxResults=1,\n",
    "                                                SortBy='CreationTime',\n",
    "                                                SortOrder='Descending')\n",
    "\n",
    "training_job_name=TrainingJobName=latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "container_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "# create a model.\n",
    "def create_model(role, model_name, container_uri, model_data):\n",
    "    return sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "        'Image': container_uri,\n",
    "        'ModelDataUrl': model_data,\n",
    "        },\n",
    "        ExecutionRoleArn=role)\n",
    "    \n",
    "\n",
    "try:\n",
    "    model = create_model(role, training_job_name, container_uri, model_data)\n",
    "except Exception as e:\n",
    "        sm_client.delete_model(ModelName=training_job_name)\n",
    "        model = create_model(role, training_job_name, container_uri, model_data)\n",
    "        \n",
    "\n",
    "print('Model created: '+model['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5091699-637f-4390-9b23-e96ffb05bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True,\n",
    "    \"InitialSamplingPercentage\": 100,\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path,\n",
    "    \"CaptureOptions\": [\n",
    "        { \"CaptureMode\": \"Output\" },\n",
    "        { \"CaptureMode\": \"Input\" }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"],\n",
    "       \"JsonContentTypes\": [\"application/json\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a059003-6625-4cdf-9e72-7e909fc72016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration created: arn:aws:sagemaker:ap-southeast-1:438465157691:endpoint-config/cc-training-job-1741689296\n"
     ]
    }
   ],
   "source": [
    "def create_endpoint_config(model_config, data_capture_config): \n",
    "    return sm_client.create_endpoint_config(\n",
    "                                                EndpointConfigName=model_config,\n",
    "                                                ProductionVariants=[\n",
    "                                                        {\n",
    "                                                            'VariantName': 'AllTraffic',\n",
    "                                                            'ModelName': model_config,\n",
    "                                                            'InitialInstanceCount': 1,\n",
    "                                                            'InstanceType': 'ml.m4.xlarge',\n",
    "                                                            'InitialVariantWeight': 1.0,\n",
    "                                                },\n",
    "                                                    \n",
    "                                                    ],\n",
    "                                                DataCaptureConfig=data_capture_config\n",
    "                                                )\n",
    "try:\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint)\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "\n",
    "print('Endpoint configuration created: '+ endpoint_config['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bc06664-d570-4094-b593-3d18d1d92ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created: arn:aws:sagemaker:ap-southeast-1:438465157691:endpoint/cc-training-job-1741689296\n"
     ]
    }
   ],
   "source": [
    "# Enable data capture, sampling 100% of the data for now. Next we deploy the endpoint in the correct VPC.\n",
    "\n",
    "endpoint_name = training_job_name\n",
    "def create_endpoint(endpoint_name, config_name):\n",
    "    return sm_client.create_endpoint(\n",
    "                                    EndpointName=endpoint_name,\n",
    "                                    EndpointConfigName=training_job_name\n",
    "                                )\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "\n",
    "print('Endpoint created: '+ endpoint['EndpointArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c99c267-1ff2-471f-bef8-c741570d7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 test_data.csv > test_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60b50575-e1a1-49f4-a5e9-b09df72ce4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Send some inference requests to this endpoint\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, content_type = 'text/csv')\n",
    "\n",
    "with open('test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload[2:])\n",
    "        sleep(0.5)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2738d30c-dd96-4f6e-8197-bbfc0ee2e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1741689296/AllTraffic\n",
      "Found Capture Files:\n",
      "sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1741689296/AllTraffic/2025/03/11/12/09-57-978-4826b0bb-8c1f-41b9-81b5-f9e7e8afc77d.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1741689296/AllTraffic/2025/03/11/12/09-57-978-4826b0bb-8c1f-41b9-81b5-f9e7e8afc77d.jsonl'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the following code to verify that Model Monitor is correctly capturing the incoming data.\n",
    "data_capture_prefix = '{}/monitoring'.format(prefix)\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/datacapture/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\n",
    "print(current_endpoint_capture_prefix)\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "\n",
    "capture_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a0b1c306-7d77-4031-a38f-e58e409d160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"data\": \"-0.34147611300851444,0.1932005252116958,50000.0,1.0,2.0,2.0,25.0,-1.0,3.0,2.0,0.0,0.0,0.0,10386.0,9993.0,9993.0,15300.0,0.0,0.0,200.0,5307.0,0.0,0.0\",\n",
      "      \"encoding\": \"CSV\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"observedContentType\": \"text/csv\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"data\": \"0.5108723044395447\",\n",
      "      \"encoding\": \"CSV\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"3d2ca48a-1d34-41b9-b35f-74e938b4f628\",\n",
      "    \"inferenceTime\": \"2025-03-11T12:10:00Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# View contents of the captured file.\n",
    "def get_obj_body(bucket, obj_key):\n",
    "    return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(rawbucket, capture_files[0])\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "661f35f5-f408-46a5-aec8-1e6be9bf9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/cc-training-job-1741689296/baselining/data\n",
      "Baseline results uri: s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/cc-training-job-1741689296/baselining/results\n",
      "s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/train_headers\n"
     ]
    }
   ],
   "source": [
    "#  Monitor the endpoint with SageMaker Model Monitor\n",
    "\n",
    "model_prefix = prefix + \"/\" + endpoint_name\n",
    "baseline_prefix = model_prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(rawbucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(rawbucket, baseline_results_prefix)\n",
    "train_data_header_location = \"s3://\" + rawbucket + '/' + prefix + '/train_headers'\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "print(train_data_header_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f465343-9cc8-4b3e-82b9-a1a5de414ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-03-11-12-13-16-111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2025-03-11-12-13-16-111\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'S3Input': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/train_headers/train_data_with_headers.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'S3Output': {'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/cc-training-job-1741689296/baselining/results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "............\u001b[34m2025-03-11 12:15:03.638078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:03.638106: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:05.199201: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:05.199228: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:05.199246: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-166-91.ap-southeast-1.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:05.199517: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,751 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:ap-southeast-1:438465157691:processing-job/baseline-suggestion-job-2025-03-11-12-13-16-111', 'ProcessingJobName': 'baseline-suggestion-job-2025-03-11-12-13-16-111', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '245545462676.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/train_headers/train_data_with_headers.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/cc-training-job-1741689296/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::438465157691:role/service-role/AmazonSageMaker-ExecutionRole-20250306T154221', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,751 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,751 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,751 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,752 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,752 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,817 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,818 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,818 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,827 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,827 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:06,827 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,275 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.166.91\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-\u001b[0m\n",
      "\u001b[34myarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,282 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,285 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-48e40329-4354-4940-88a2-69b40982e1bd\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,843 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,857 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,859 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,861 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,866 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,866 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,866 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,866 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,897 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,907 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,907 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,911 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,914 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Mar 11 12:15:07\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,915 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,915 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,916 INFO util.GSet: 2.0% max memory 3.1 GB = 63.5 MB\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,917 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,990 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,993 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:07,994 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,019 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,019 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,019 INFO util.GSet: 1.0% max memory 3.1 GB = 31.7 MB\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,019 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,021 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,021 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,021 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,021 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,025 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,029 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,029 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,029 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,029 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,035 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,035 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,035 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,038 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,038 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,039 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,039 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,040 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 974.9 KB\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,040 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,059 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1969199386-10.0.166.91-1741695308053\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,073 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,080 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,157 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,169 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,173 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.166.91\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:08,184 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:10,242 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:10,243 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:12,312 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:12,312 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:14,390 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:14,391 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:16,479 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:16,479 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:18,575 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:18,576 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:28,585 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:30,256 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:30,709 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:30,745 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:30,755 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,252 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,277 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,277 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,278 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,278 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,302 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,315 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,316 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,366 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,367 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,367 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,367 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,368 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,763 INFO util.Utils: Successfully started service 'sparkDriver' on port 35603.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,796 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,832 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,854 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,854 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,885 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,905 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-71772cb5-0ee6-4a42-ad25-b7ef8783c320\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,921 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,958 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:31,990 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.166.91:35603/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1741695331247\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:32,455 INFO client.RMProxy: Connecting to ResourceManager at /10.0.166.91:8032\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,131 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,131 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,137 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,138 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,138 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,138 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,144 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:33,220 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:35,505 INFO yarn.Client: Uploading resource file:/tmp/spark-3a81d139-8cab-400f-8bd6-528a37caf7fb/__spark_libs__7916022529959289859.zip -> hdfs://10.0.166.91/user/root/.sparkStaging/application_1741695313800_0001/__spark_libs__7916022529959289859.zip\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,660 INFO yarn.Client: Uploading resource file:/tmp/spark-3a81d139-8cab-400f-8bd6-528a37caf7fb/__spark_conf__3918117944056720338.zip -> hdfs://10.0.166.91/user/root/.sparkStaging/application_1741695313800_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,709 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,709 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,709 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,709 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,709 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,736 INFO yarn.Client: Submitting application application_1741695313800_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:36,926 INFO impl.YarnClientImpl: Submitted application application_1741695313800_0001\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:37,930 INFO yarn.Client: Application report for application_1741695313800_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:37,933 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Tue Mar 11 12:15:37 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1741695336833\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1741695313800_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:38,936 INFO yarn.Client: Application report for application_1741695313800_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:39,938 INFO yarn.Client: Application report for application_1741695313800_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:40,941 INFO yarn.Client: Application report for application_1741695313800_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:41,945 INFO yarn.Client: Application report for application_1741695313800_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,307 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1741695313800_0001), /proxy/application_1741695313800_0001\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,948 INFO yarn.Client: Application report for application_1741695313800_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,949 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.166.91\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1741695336833\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1741695313800_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,950 INFO cluster.YarnClientSchedulerBackend: Application application_1741695313800_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,989 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34719.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,989 INFO netty.NettyBlockTransferService: Server created on 10.0.166.91:34719\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:42,992 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:43,016 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.166.91, 34719, None)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:43,020 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.166.91:34719 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.166.91, 34719, None)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:43,025 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.166.91, 34719, None)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:43,026 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.166.91, 34719, None)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:43,199 INFO util.log: Logging initialized @14371ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:43,896 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:48,187 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.166.91:52702) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:15:48,370 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:36959 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 36959, None)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:02,361 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:02,589 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:02,640 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:02,644 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:03,672 INFO datasources.InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:03,831 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,125 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,131 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.166.91:34719 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,140 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,506 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,508 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,512 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 3887463\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,578 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,598 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,599 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,599 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,601 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,607 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,634 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,639 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,643 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.166.91:34719 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,644 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,663 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,664 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,704 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4636 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:04,898 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:36959 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,585 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:36959 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,898 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1207 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,900 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,909 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.281 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,912 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,913 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:05,914 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.335643 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:06,089 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.166.91:34719 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:06,092 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:36959 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,064 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,066 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,070 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Label: string, PAY_AMT1: string, BILL_AMT1: string, LIMIT_BAL: string, SEX: string ... 22 more fields>\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,266 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,279 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,280 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.166.91:34719 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,281 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,293 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,328 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,329 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,330 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,330 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,332 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,333 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,382 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,384 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,385 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.166.91:34719 (size: 8.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,386 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,387 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,387 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,391 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:08,435 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:36959 (size: 8.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:09,344 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:36959 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,136 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:36959 (size: 3.2 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,270 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1882 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,270 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,271 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.934 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,272 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,272 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,272 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.943747 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,573 INFO codegen.CodeGenerator: Code generated in 203.392974 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,727 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.166.91:34719 in memory (size: 8.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:10,729 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:36959 in memory (size: 8.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,107 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,235 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,239 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,239 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,239 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,241 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,243 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,268 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,270 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,271 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.166.91:34719 (size: 35.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,272 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,274 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,274 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,282 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:11,312 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:36959 (size: 35.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,744 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1464 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,744 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,748 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.501 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,748 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,748 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,749 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,749 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,852 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,855 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,855 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,855 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,855 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,857 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,871 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,873 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,874 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.166.91:34719 (size: 46.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,874 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,875 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,875 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,878 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,895 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:36959 (size: 46.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:12,942 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,320 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 443 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,326 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.462 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,327 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,327 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,327 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,328 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.474949 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,359 INFO codegen.CodeGenerator: Code generated in 22.164635 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,618 INFO codegen.CodeGenerator: Code generated in 31.690239 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,694 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,695 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,696 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,696 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,697 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,698 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,715 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,717 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,718 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.166.91:34719 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,718 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,719 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,719 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,721 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:13,735 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:36959 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:14,815 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1094 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:14,815 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:14,816 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.115 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:14,816 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:14,816 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:14,817 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.123078 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,335 INFO codegen.CodeGenerator: Code generated in 106.244113 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,344 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,344 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,344 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,344 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,345 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,346 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,350 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 76.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,352 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,353 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.166.91:34719 (size: 24.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,353 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,353 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,354 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,355 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,373 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:36959 (size: 24.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,595 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 240 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,595 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,596 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.249 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,596 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,596 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,596 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,596 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,714 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:36959 in memory (size: 35.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,720 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.166.91:34719 in memory (size: 35.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,750 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.166.91:34719 in memory (size: 24.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,754 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:36959 in memory (size: 24.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,767 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:36959 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,767 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.166.91:34719 in memory (size: 17.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,779 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:36959 in memory (size: 46.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,798 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.166.91:34719 in memory (size: 46.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,856 INFO codegen.CodeGenerator: Code generated in 153.189165 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,871 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,873 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,873 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,873 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,874 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,875 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,877 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,879 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,880 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.166.91:34719 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,880 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,881 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,881 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,882 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,895 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:36959 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:15,898 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,024 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 142 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,024 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,025 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,026 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,026 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,027 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.155204 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,141 INFO codegen.CodeGenerator: Code generated in 66.234587 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,312 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,323 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,324 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,324 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,324 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,325 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,328 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,343 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,345 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,346 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.166.91:34719 (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,351 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,351 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,351 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,353 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:16,366 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:36959 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,608 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1255 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,608 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,609 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.281 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,610 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,611 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,611 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,611 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,612 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,615 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,617 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,617 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.166.91:34719 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,618 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,619 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,619 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,621 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,636 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:36959 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,647 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,687 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 67 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,687 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,688 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.075 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,688 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,688 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,688 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.376472 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,900 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,900 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,900 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,900 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,900 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,901 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,906 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 85.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,907 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,908 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.166.91:34719 (size: 27.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,908 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,909 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,909 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,911 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:17,921 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:36959 (size: 27.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,127 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 217 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,128 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,129 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.226 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,129 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,129 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,129 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,129 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,163 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,165 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,165 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,165 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,165 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,166 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,179 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 170.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,182 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,182 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.166.91:34719 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,183 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,184 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,184 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,185 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,199 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:36959 (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,209 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,280 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 95 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,280 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,286 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,287 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,287 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,288 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.124159 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,394 INFO codegen.CodeGenerator: Code generated in 16.1461 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,427 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,428 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,429 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,429 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,429 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,431 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,439 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 40.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,441 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,441 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.166.91:34719 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,442 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,442 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,442 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,444 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:18,456 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:36959 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,109 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 666 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,110 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.678 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,111 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,112 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,112 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,113 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.685190 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,365 INFO codegen.CodeGenerator: Code generated in 54.977026 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,372 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,373 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,373 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,373 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,373 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,374 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,377 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 75.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,379 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,380 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.166.91:34719 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,380 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,380 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,380 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,382 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,395 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:36959 (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,504 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,504 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,504 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.129 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,505 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,505 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,505 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,505 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,695 INFO codegen.CodeGenerator: Code generated in 105.183945 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,707 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,708 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,709 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,709 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,709 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,710 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,712 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,714 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,714 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.166.91:34719 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,715 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,715 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,715 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,717 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,730 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:36959 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,737 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,856 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,856 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,859 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.148 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,859 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,859 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:19,859 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.152116 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,036 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:36959 in memory (size: 27.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,047 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.166.91:34719 in memory (size: 27.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,057 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,068 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,069 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,069 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,070 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,070 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,073 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,127 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,129 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,130 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.166.91:34719 (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,130 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,131 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,131 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,132 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,163 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:36959 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,165 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.166.91:34719 in memory (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,173 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:36959 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,279 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.166.91:34719 in memory (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,292 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:36959 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,381 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.166.91:34719 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,397 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:36959 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,416 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:36959 in memory (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,420 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.166.91:34719 in memory (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,438 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:36959 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,443 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.166.91:34719 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,446 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 314 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,446 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,447 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.373 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,447 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,447 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,447 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,447 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,448 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,450 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,451 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,452 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.166.91:34719 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,453 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,453 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,454 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,455 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,468 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:36959 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,471 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:36959 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,473 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,484 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.166.91:34719 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,494 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,494 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,495 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,497 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,497 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,497 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.430843 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,505 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.166.91:34719 in memory (size: 46.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,512 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:36959 in memory (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,671 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,671 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,672 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,672 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,673 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,673 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,678 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,680 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,680 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.166.91:34719 (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,681 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,681 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,682 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,683 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,694 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:36959 (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,915 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 232 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,915 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,916 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.241 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,916 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,916 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,916 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,916 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,949 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,950 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,950 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,950 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,950 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,950 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,956 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 170.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,958 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,958 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.166.91:34719 (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,958 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,959 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,959 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,960 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,969 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:36959 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:20,976 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,073 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 113 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,073 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,074 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.123 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,075 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,075 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,075 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.126536 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,165 INFO codegen.CodeGenerator: Code generated in 14.721292 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,198 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,199 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,199 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,199 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,200 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,200 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,206 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 40.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,208 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,209 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.166.91:34719 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,210 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,210 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,210 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,212 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,226 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:36959 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,752 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 539 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,752 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,753 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.551 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,753 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,754 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,754 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.555998 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,976 INFO codegen.CodeGenerator: Code generated in 52.009581 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,983 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,984 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,984 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,984 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,985 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,986 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,995 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 75.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,997 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,997 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.166.91:34719 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:21,998 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,002 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,002 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,004 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,021 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:36959 (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,209 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 206 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,209 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,209 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.222 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,210 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,210 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,210 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,210 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,265 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,266 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,266 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,267 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,267 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,268 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,270 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,273 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,273 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.166.91:34719 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,274 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,274 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,274 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,276 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,288 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:36959 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,292 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,300 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,300 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,301 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,301 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,302 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,302 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.036766 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,361 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,362 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,363 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,363 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,363 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,364 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,366 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,372 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,374 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,374 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.166.91:34719 (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,375 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,375 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,376 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,378 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,397 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:36959 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,618 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 240 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,618 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,620 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.253 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,620 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,620 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,621 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,621 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,621 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,625 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,627 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,629 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.166.91:34719 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,630 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,630 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,630 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,631 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,643 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:36959 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,646 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,659 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,659 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,659 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,660 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,660 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,661 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.299283 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,750 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,751 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,751 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,751 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,752 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,752 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,756 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 85.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,758 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,758 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.166.91:34719 (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,759 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,759 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,759 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,760 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,769 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:36959 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,932 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 172 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,932 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,933 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.179 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,933 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,933 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,933 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,933 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,959 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,960 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,960 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,960 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,960 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,961 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,965 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 170.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,986 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,987 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.166.91:34719 (size: 46.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,992 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,992 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,992 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,992 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:36959 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,994 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:22,994 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.166.91:34719 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,004 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.166.91:34719 in memory (size: 27.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,005 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:36959 in memory (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,008 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:36959 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,010 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:36959 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,021 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,027 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.166.91:34719 in memory (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,034 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.166.91:34719 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,035 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:36959 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,047 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.166.91:34719 in memory (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,060 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:36959 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,092 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:36959 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,093 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.166.91:34719 in memory (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,126 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.166.91:34719 in memory (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,130 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:36959 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,149 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 156 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,150 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,150 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.189 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,151 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.166.91:34719 in memory (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,151 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,151 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,152 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.192394 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,154 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:36959 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,191 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.166.91:34719 in memory (size: 24.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,192 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:36959 in memory (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,247 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:36959 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,249 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.166.91:34719 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,284 INFO codegen.CodeGenerator: Code generated in 21.64779 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,336 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,337 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,338 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,338 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,339 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,339 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,348 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 40.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,350 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,350 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.166.91:34719 (size: 16.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,351 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,351 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,352 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,353 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,362 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:36959 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,951 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 598 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,952 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.611 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,952 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,952 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,953 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:23,953 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.616238 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,192 INFO codegen.CodeGenerator: Code generated in 76.607134 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,204 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,205 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,205 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,205 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,206 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,206 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,227 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 75.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,234 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,235 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.166.91:34719 (size: 23.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,235 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,236 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,236 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,238 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,249 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:36959 (size: 23.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,404 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 167 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,404 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,404 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.197 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,404 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,404 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,404 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,405 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,528 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,531 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,532 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,532 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,532 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,533 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,535 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,537 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,537 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.166.91:34719 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,538 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,538 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,538 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,540 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,552 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:36959 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,558 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,573 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,573 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,574 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.040 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,574 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,575 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,575 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.044189 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,918 INFO scheduler.DAGScheduler: Registering RDD 147 (collect at AnalysisRunner.scala:326) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,919 INFO scheduler.DAGScheduler: Got map stage job 25 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,919 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,919 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,920 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,920 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,924 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 75.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,925 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,926 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.166.91:34719 (size: 25.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,926 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,927 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,927 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,928 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:24,941 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:36959 (size: 25.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,297 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 369 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,297 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,298 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326) finished in 0.377 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,298 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,298 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,298 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,298 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,327 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,328 INFO scheduler.DAGScheduler: Got job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,329 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,329 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,329 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,329 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,335 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 143.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,337 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 40.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,337 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.166.91:34719 (size: 40.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,338 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,338 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,338 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,340 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,350 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:36959 (size: 40.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,359 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,447 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 29) in 108 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,447 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,448 INFO scheduler.DAGScheduler: ResultStage 38 (collect at AnalysisRunner.scala:326) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,449 INFO scheduler.DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,449 INFO cluster.YarnScheduler: Killing all running tasks in stage 38: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,449 INFO scheduler.DAGScheduler: Job 26 finished: collect at AnalysisRunner.scala:326, took 0.121622 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,469 INFO codegen.CodeGenerator: Code generated in 16.884735 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,580 INFO codegen.CodeGenerator: Code generated in 21.949786 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,609 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,610 INFO scheduler.DAGScheduler: Got job 27 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,610 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,611 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,611 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,612 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[160] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,618 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 39.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,621 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,622 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.166.91:34719 (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,625 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,625 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[160] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,626 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,627 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:25,637 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:36959 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,143 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 30) in 516 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,145 INFO scheduler.DAGScheduler: ResultStage 39 (treeReduce at KLLRunner.scala:107) finished in 0.531 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,145 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,150 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,150 INFO cluster.YarnScheduler: Killing all running tasks in stage 39: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,151 INFO scheduler.DAGScheduler: Job 27 finished: treeReduce at KLLRunner.scala:107, took 0.541305 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,308 INFO codegen.CodeGenerator: Code generated in 46.405771 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,320 INFO scheduler.DAGScheduler: Registering RDD 165 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,320 INFO scheduler.DAGScheduler: Got map stage job 28 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,321 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,321 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,322 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,322 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[165] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,327 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 65.5 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,331 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,332 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.166.91:34719 (size: 21.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,332 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,333 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[165] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,333 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,336 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,354 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:36959 (size: 21.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,513 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 176 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,513 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,514 INFO scheduler.DAGScheduler: ShuffleMapStage 40 (collect at AnalysisRunner.scala:326) finished in 0.190 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,514 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,514 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,515 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,515 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,694 INFO codegen.CodeGenerator: Code generated in 107.540605 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,713 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,714 INFO scheduler.DAGScheduler: Got job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,715 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,715 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,715 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,716 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[168] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,718 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 55.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,719 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,720 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.166.91:34719 (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,720 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,721 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[168] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,721 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,723 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 32) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,732 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:36959 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,736 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,809 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 32) in 87 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,811 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,812 INFO scheduler.DAGScheduler: ResultStage 42 (collect at AnalysisRunner.scala:326) finished in 0.096 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,812 INFO scheduler.DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,812 INFO cluster.YarnScheduler: Killing all running tasks in stage 42: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,813 INFO scheduler.DAGScheduler: Job 29 finished: collect at AnalysisRunner.scala:326, took 0.098984 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,878 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.166.91:34719 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,883 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:36959 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,896 INFO codegen.CodeGenerator: Code generated in 75.127261 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,908 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.166.91:34719 in memory (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,909 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:36959 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,913 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.166.91:34719 in memory (size: 23.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,914 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:36959 in memory (size: 23.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,916 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.166.91:34719 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,918 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:36959 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,922 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.166.91:34719 in memory (size: 25.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,923 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:36959 in memory (size: 25.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,928 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.166.91:34719 in memory (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,929 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:36959 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,939 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.166.91:34719 in memory (size: 40.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,939 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:36959 in memory (size: 40.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,942 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.166.91:34719 in memory (size: 21.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,942 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:36959 in memory (size: 21.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,945 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:36959 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:26,951 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.166.91:34719 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,250 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,292 INFO codegen.CodeGenerator: Code generated in 9.783034 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,298 INFO scheduler.DAGScheduler: Registering RDD 173 (count at StatsGenerator.scala:66) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,298 INFO scheduler.DAGScheduler: Got map stage job 30 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,298 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 43 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,298 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,299 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,299 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[173] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,304 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 24.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,306 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,307 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.166.91:34719 (size: 10.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,309 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,310 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[173] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,310 INFO cluster.YarnScheduler: Adding task set 43.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,312 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,321 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:36959 (size: 10.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,388 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 33) in 76 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,389 INFO cluster.YarnScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,389 INFO scheduler.DAGScheduler: ShuffleMapStage 43 (count at StatsGenerator.scala:66) finished in 0.088 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,390 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,391 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,391 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,391 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,432 INFO codegen.CodeGenerator: Code generated in 23.003281 ms\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,454 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,459 INFO scheduler.DAGScheduler: Got job 31 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,459 INFO scheduler.DAGScheduler: Final stage: ResultStage 45 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,459 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,459 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,459 INFO scheduler.DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[176] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,461 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 11.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,462 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,463 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.166.91:34719 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,463 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,466 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[176] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,466 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,467 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,481 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:36959 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,485 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.166.91:52702\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,521 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 34) in 54 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,521 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,522 INFO scheduler.DAGScheduler: ResultStage 45 (count at StatsGenerator.scala:66) finished in 0.061 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,522 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,522 INFO cluster.YarnScheduler: Killing all running tasks in stage 45: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:27,523 INFO scheduler.DAGScheduler: Job 31 finished: count at StatsGenerator.scala:66, took 0.064806 s\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,237 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,259 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,288 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,290 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,301 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,319 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,398 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,413 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,422 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,425 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,479 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,479 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,479 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,511 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,512 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5c947554-91af-47b0-a3a3-886168ee34d3\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,516 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3a81d139-8cab-400f-8bd6-528a37caf7fb\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,644 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-03-11 12:16:28,645 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7fbd491f36d0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up a baseline job for Model Monitor to capture the statistics of the training data\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600)\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=os.path.join(train_data_header_location, 'train_data_with_headers.csv'),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f0aaacd-08b8-48bb-96e5-a4343df09156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker-modelmonitor/cc-training-job-1741689296/baselining/results/constraints.json\n",
      " sagemaker-modelmonitor/cc-training-job-1741689296/baselining/results/statistics.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.approximate_num_distinct_values</th>\n",
       "      <th>numerical_statistics.completeness</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>Integral</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.224583e-01</td>\n",
       "      <td>5.339000e+03</td>\n",
       "      <td>0.415897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.965234e-16</td>\n",
       "      <td>4.716560e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>5.223016e+01</td>\n",
       "      <td>6922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -0.34147611300851444, 'upper_...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.16093173468294658, -0.2812946535666585, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.378091e-01</td>\n",
       "      <td>5.707419e+03</td>\n",
       "      <td>0.080585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>17990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.0999999...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.19552189076210494, 0.2369590330493186, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.677310e+05</td>\n",
       "      <td>4.025544e+09</td>\n",
       "      <td>129479.698677</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 10000.0, 'upper_bound': 89000...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[30000.0, 120000.0, 200000.0, 130000.0, 28000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.601167e+00</td>\n",
       "      <td>3.842800e+04</td>\n",
       "      <td>0.489658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 1.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.851083e+00</td>\n",
       "      <td>4.442600e+04</td>\n",
       "      <td>0.788030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.6, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.553125e+00</td>\n",
       "      <td>3.727500e+04</td>\n",
       "      <td>0.521227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.3, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.545954e+01</td>\n",
       "      <td>8.510290e+05</td>\n",
       "      <td>9.191179</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.900000e+01</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 21.0, 'upper_bound': 26.8, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[26.0, 27.0, 58.0, 29.0, 49.0, 23.0, 26.0, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAY_0</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.316667e-02</td>\n",
       "      <td>-3.160000e+02</td>\n",
       "      <td>1.127531</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, 0.0, 1.0, 0.0, -2.0, -1.0, -1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.276667e-01</td>\n",
       "      <td>-3.064000e+03</td>\n",
       "      <td>1.199702</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.1, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.595833e-01</td>\n",
       "      <td>-3.830000e+03</td>\n",
       "      <td>1.198833</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, -1.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.157500e-01</td>\n",
       "      <td>-5.178000e+03</td>\n",
       "      <td>1.168026</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, -1.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.625833e-01</td>\n",
       "      <td>-6.302000e+03</td>\n",
       "      <td>1.130582</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.858750e-01</td>\n",
       "      <td>-6.861000e+03</td>\n",
       "      <td>1.148760</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.929893e+04</td>\n",
       "      <td>1.183174e+09</td>\n",
       "      <td>70960.574221</td>\n",
       "      <td>-69777.000000</td>\n",
       "      <td>7.439700e+05</td>\n",
       "      <td>17467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -69777.0, 'upper_bound': 1159...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[14899.0, 15000.0, 206084.0, 67664.0, 6163.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.717089e+04</td>\n",
       "      <td>1.132101e+09</td>\n",
       "      <td>69505.298901</td>\n",
       "      <td>-157264.000000</td>\n",
       "      <td>1.664089e+06</td>\n",
       "      <td>18307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -157264.0, 'upper_bound': 248...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[16790.0, 100.0, 187747.0, 66258.0, -54.0, 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.325871e+04</td>\n",
       "      <td>1.038209e+09</td>\n",
       "      <td>64153.274849</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>7.068640e+05</td>\n",
       "      <td>19323</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -170000.0, 'upper_bound': -82...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[12400.0, 0.0, 159746.0, 62697.0, -54.0, 4536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.036543e+04</td>\n",
       "      <td>9.687702e+08</td>\n",
       "      <td>60673.598646</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>8.235400e+05</td>\n",
       "      <td>17343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -81334.0, 'upper_bound': 9153...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[780.0, 0.0, 140632.0, 48210.0, -54.0, 46135....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.891905e+04</td>\n",
       "      <td>9.340573e+08</td>\n",
       "      <td>59424.681250</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>6.999440e+05</td>\n",
       "      <td>16148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': -339603.0, 'upper_bound': -23...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 143092.0, 46255.0, -54.0, 46029.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.926893e+03</td>\n",
       "      <td>1.422454e+08</td>\n",
       "      <td>23436.624063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>6647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 168425.9,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[3000.0, 100.0, 0.0, 3000.0, 0.0, 45361.0, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.186248e+03</td>\n",
       "      <td>1.244700e+08</td>\n",
       "      <td>17186.818924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.890430e+05</td>\n",
       "      <td>6796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 88904.3, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[6000.0, 0.0, 6014.0, 6000.0, 0.0, 1800.0, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.830179e+03</td>\n",
       "      <td>1.159243e+08</td>\n",
       "      <td>15482.205461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.210000e+05</td>\n",
       "      <td>5931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 62100.0, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[780.0, 0.0, 5000.0, 2000.0, 0.0, 1503.0, 706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.797411e+03</td>\n",
       "      <td>1.151379e+08</td>\n",
       "      <td>15166.388154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.880710e+05</td>\n",
       "      <td>5553</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 38807.1, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 6000.0, 2000.0, 0.0, 1735.0, 954.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.219489e+03</td>\n",
       "      <td>1.252677e+08</td>\n",
       "      <td>17606.700346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.271430e+05</td>\n",
       "      <td>6218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 52714.3, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 5000.0, 2000.0, 0.0, 1724.0, 700.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0       Label      Integral                                    24000   \n",
       "1    PAY_AMT1    Fractional                                    24000   \n",
       "2   BILL_AMT1    Fractional                                    24000   \n",
       "3   LIMIT_BAL    Fractional                                    24000   \n",
       "4         SEX    Fractional                                    24000   \n",
       "5   EDUCATION    Fractional                                    24000   \n",
       "6    MARRIAGE    Fractional                                    24000   \n",
       "7         AGE    Fractional                                    24000   \n",
       "8       PAY_0    Fractional                                    24000   \n",
       "9       PAY_2    Fractional                                    24000   \n",
       "10      PAY_3    Fractional                                    24000   \n",
       "11      PAY_4    Fractional                                    24000   \n",
       "12      PAY_5    Fractional                                    24000   \n",
       "13      PAY_6    Fractional                                    24000   \n",
       "14  BILL_AMT2    Fractional                                    24000   \n",
       "15  BILL_AMT3    Fractional                                    24000   \n",
       "16  BILL_AMT4    Fractional                                    24000   \n",
       "17  BILL_AMT5    Fractional                                    24000   \n",
       "18  BILL_AMT6    Fractional                                    24000   \n",
       "19   PAY_AMT2    Fractional                                    24000   \n",
       "20   PAY_AMT3    Fractional                                    24000   \n",
       "21   PAY_AMT4    Fractional                                    24000   \n",
       "22   PAY_AMT5    Fractional                                    24000   \n",
       "23   PAY_AMT6    Fractional                                    24000   \n",
       "\n",
       "    numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                         0               2.224583e-01   \n",
       "1                                         0               1.965234e-16   \n",
       "2                                         0               2.378091e-01   \n",
       "3                                         0               1.677310e+05   \n",
       "4                                         0               1.601167e+00   \n",
       "5                                         0               1.851083e+00   \n",
       "6                                         0               1.553125e+00   \n",
       "7                                         0               3.545954e+01   \n",
       "8                                         0              -1.316667e-02   \n",
       "9                                         0              -1.276667e-01   \n",
       "10                                        0              -1.595833e-01   \n",
       "11                                        0              -2.157500e-01   \n",
       "12                                        0              -2.625833e-01   \n",
       "13                                        0              -2.858750e-01   \n",
       "14                                        0               4.929893e+04   \n",
       "15                                        0               4.717089e+04   \n",
       "16                                        0               4.325871e+04   \n",
       "17                                        0               4.036543e+04   \n",
       "18                                        0               3.891905e+04   \n",
       "19                                        0               5.926893e+03   \n",
       "20                                        0               5.186248e+03   \n",
       "21                                        0               4.830179e+03   \n",
       "22                                        0               4.797411e+03   \n",
       "23                                        0               5.219489e+03   \n",
       "\n",
       "    numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0               5.339000e+03                      0.415897   \n",
       "1               4.716560e-12                      1.000000   \n",
       "2               5.707419e+03                      0.080585   \n",
       "3               4.025544e+09                 129479.698677   \n",
       "4               3.842800e+04                      0.489658   \n",
       "5               4.442600e+04                      0.788030   \n",
       "6               3.727500e+04                      0.521227   \n",
       "7               8.510290e+05                      9.191179   \n",
       "8              -3.160000e+02                      1.127531   \n",
       "9              -3.064000e+03                      1.199702   \n",
       "10             -3.830000e+03                      1.198833   \n",
       "11             -5.178000e+03                      1.168026   \n",
       "12             -6.302000e+03                      1.130582   \n",
       "13             -6.861000e+03                      1.148760   \n",
       "14              1.183174e+09                  70960.574221   \n",
       "15              1.132101e+09                  69505.298901   \n",
       "16              1.038209e+09                  64153.274849   \n",
       "17              9.687702e+08                  60673.598646   \n",
       "18              9.340573e+08                  59424.681250   \n",
       "19              1.422454e+08                  23436.624063   \n",
       "20              1.244700e+08                  17186.818924   \n",
       "21              1.159243e+08                  15482.205461   \n",
       "22              1.151379e+08                  15166.388154   \n",
       "23              1.252677e+08                  17606.700346   \n",
       "\n",
       "    numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                   0.000000              1.000000e+00   \n",
       "1                  -0.341476              5.223016e+01   \n",
       "2                   0.000000              1.000000e+00   \n",
       "3               10000.000000              8.000000e+05   \n",
       "4                   1.000000              2.000000e+00   \n",
       "5                   0.000000              6.000000e+00   \n",
       "6                   0.000000              3.000000e+00   \n",
       "7                  21.000000              7.900000e+01   \n",
       "8                  -2.000000              8.000000e+00   \n",
       "9                  -2.000000              7.000000e+00   \n",
       "10                 -2.000000              8.000000e+00   \n",
       "11                 -2.000000              8.000000e+00   \n",
       "12                 -2.000000              8.000000e+00   \n",
       "13                 -2.000000              8.000000e+00   \n",
       "14             -69777.000000              7.439700e+05   \n",
       "15            -157264.000000              1.664089e+06   \n",
       "16            -170000.000000              7.068640e+05   \n",
       "17             -81334.000000              8.235400e+05   \n",
       "18            -339603.000000              6.999440e+05   \n",
       "19                  0.000000              1.684259e+06   \n",
       "20                  0.000000              8.890430e+05   \n",
       "21                  0.000000              6.210000e+05   \n",
       "22                  0.000000              3.880710e+05   \n",
       "23                  0.000000              5.271430e+05   \n",
       "\n",
       "    numerical_statistics.approximate_num_distinct_values  \\\n",
       "0                                                   2      \n",
       "1                                                6922      \n",
       "2                                               17990      \n",
       "3                                                  81      \n",
       "4                                                   2      \n",
       "5                                                   7      \n",
       "6                                                   4      \n",
       "7                                                  55      \n",
       "8                                                  11      \n",
       "9                                                  10      \n",
       "10                                                 11      \n",
       "11                                                 11      \n",
       "12                                                 10      \n",
       "13                                                 10      \n",
       "14                                              17467      \n",
       "15                                              18307      \n",
       "16                                              19323      \n",
       "17                                              17343      \n",
       "18                                              16148      \n",
       "19                                               6647      \n",
       "20                                               6796      \n",
       "21                                               5931      \n",
       "22                                               5553      \n",
       "23                                               6218      \n",
       "\n",
       "    numerical_statistics.completeness  \\\n",
       "0                                 1.0   \n",
       "1                                 1.0   \n",
       "2                                 1.0   \n",
       "3                                 1.0   \n",
       "4                                 1.0   \n",
       "5                                 1.0   \n",
       "6                                 1.0   \n",
       "7                                 1.0   \n",
       "8                                 1.0   \n",
       "9                                 1.0   \n",
       "10                                1.0   \n",
       "11                                1.0   \n",
       "12                                1.0   \n",
       "13                                1.0   \n",
       "14                                1.0   \n",
       "15                                1.0   \n",
       "16                                1.0   \n",
       "17                                1.0   \n",
       "18                                1.0   \n",
       "19                                1.0   \n",
       "20                                1.0   \n",
       "21                                1.0   \n",
       "22                                1.0   \n",
       "23                                1.0   \n",
       "\n",
       "        numerical_statistics.distribution.kll.buckets  \\\n",
       "0   [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1   [{'lower_bound': -0.34147611300851444, 'upper_...   \n",
       "2   [{'lower_bound': 0.0, 'upper_bound': 0.0999999...   \n",
       "3   [{'lower_bound': 10000.0, 'upper_bound': 89000...   \n",
       "4   [{'lower_bound': 1.0, 'upper_bound': 1.1, 'cou...   \n",
       "5   [{'lower_bound': 0.0, 'upper_bound': 0.6, 'cou...   \n",
       "6   [{'lower_bound': 0.0, 'upper_bound': 0.3, 'cou...   \n",
       "7   [{'lower_bound': 21.0, 'upper_bound': 26.8, 'c...   \n",
       "8   [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "9   [{'lower_bound': -2.0, 'upper_bound': -1.1, 'c...   \n",
       "10  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "11  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "12  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "13  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "14  [{'lower_bound': -69777.0, 'upper_bound': 1159...   \n",
       "15  [{'lower_bound': -157264.0, 'upper_bound': 248...   \n",
       "16  [{'lower_bound': -170000.0, 'upper_bound': -82...   \n",
       "17  [{'lower_bound': -81334.0, 'upper_bound': 9153...   \n",
       "18  [{'lower_bound': -339603.0, 'upper_bound': -23...   \n",
       "19  [{'lower_bound': 0.0, 'upper_bound': 168425.9,...   \n",
       "20  [{'lower_bound': 0.0, 'upper_bound': 88904.3, ...   \n",
       "21  [{'lower_bound': 0.0, 'upper_bound': 62100.0, ...   \n",
       "22  [{'lower_bound': 0.0, 'upper_bound': 38807.1, ...   \n",
       "23  [{'lower_bound': 0.0, 'upper_bound': 52714.3, ...   \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                                0.64           \n",
       "1                                                0.64           \n",
       "2                                                0.64           \n",
       "3                                                0.64           \n",
       "4                                                0.64           \n",
       "5                                                0.64           \n",
       "6                                                0.64           \n",
       "7                                                0.64           \n",
       "8                                                0.64           \n",
       "9                                                0.64           \n",
       "10                                               0.64           \n",
       "11                                               0.64           \n",
       "12                                               0.64           \n",
       "13                                               0.64           \n",
       "14                                               0.64           \n",
       "15                                               0.64           \n",
       "16                                               0.64           \n",
       "17                                               0.64           \n",
       "18                                               0.64           \n",
       "19                                               0.64           \n",
       "20                                               0.64           \n",
       "21                                               0.64           \n",
       "22                                               0.64           \n",
       "23                                               0.64           \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                              2048.0           \n",
       "1                                              2048.0           \n",
       "2                                              2048.0           \n",
       "3                                              2048.0           \n",
       "4                                              2048.0           \n",
       "5                                              2048.0           \n",
       "6                                              2048.0           \n",
       "7                                              2048.0           \n",
       "8                                              2048.0           \n",
       "9                                              2048.0           \n",
       "10                                             2048.0           \n",
       "11                                             2048.0           \n",
       "12                                             2048.0           \n",
       "13                                             2048.0           \n",
       "14                                             2048.0           \n",
       "15                                             2048.0           \n",
       "16                                             2048.0           \n",
       "17                                             2048.0           \n",
       "18                                             2048.0           \n",
       "19                                             2048.0           \n",
       "20                                             2048.0           \n",
       "21                                             2048.0           \n",
       "22                                             2048.0           \n",
       "23                                             2048.0           \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.data  \n",
       "0   [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1   [[-0.16093173468294658, -0.2812946535666585, 0...  \n",
       "2   [[0.19552189076210494, 0.2369590330493186, 0.4...  \n",
       "3   [[30000.0, 120000.0, 200000.0, 130000.0, 28000...  \n",
       "4   [[1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,...  \n",
       "5   [[2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,...  \n",
       "6   [[2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,...  \n",
       "7   [[26.0, 27.0, 58.0, 29.0, 49.0, 23.0, 26.0, 29...  \n",
       "8   [[-1.0, 0.0, 1.0, 0.0, -2.0, -1.0, -1.0, 0.0, ...  \n",
       "9   [[0.0, 0.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 5....  \n",
       "10  [[0.0, -1.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 4...  \n",
       "11  [[0.0, -1.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 3....  \n",
       "12  [[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 2...  \n",
       "13  [[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0...  \n",
       "14  [[14899.0, 15000.0, 206084.0, 67664.0, 6163.0,...  \n",
       "15  [[16790.0, 100.0, 187747.0, 66258.0, -54.0, 44...  \n",
       "16  [[12400.0, 0.0, 159746.0, 62697.0, -54.0, 4536...  \n",
       "17  [[780.0, 0.0, 140632.0, 48210.0, -54.0, 46135....  \n",
       "18  [[0.0, 0.0, 143092.0, 46255.0, -54.0, 46029.0,...  \n",
       "19  [[3000.0, 100.0, 0.0, 3000.0, 0.0, 45361.0, 12...  \n",
       "20  [[6000.0, 0.0, 6014.0, 6000.0, 0.0, 1800.0, 65...  \n",
       "21  [[780.0, 0.0, 5000.0, 2000.0, 0.0, 1503.0, 706...  \n",
       "22  [[0.0, 0.0, 6000.0, 2000.0, 0.0, 1735.0, 954.0...  \n",
       "23  [[0.0, 0.0, 5000.0, 2000.0, 0.0, 1724.0, 700.0...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc01739d-19d8-4771-ae14-5e49d1622153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAY_0</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0       Label      Integral           1.0                             True\n",
       "1    PAY_AMT1    Fractional           1.0                            False\n",
       "2   BILL_AMT1    Fractional           1.0                             True\n",
       "3   LIMIT_BAL    Fractional           1.0                             True\n",
       "4         SEX    Fractional           1.0                             True\n",
       "5   EDUCATION    Fractional           1.0                             True\n",
       "6    MARRIAGE    Fractional           1.0                             True\n",
       "7         AGE    Fractional           1.0                             True\n",
       "8       PAY_0    Fractional           1.0                            False\n",
       "9       PAY_2    Fractional           1.0                            False\n",
       "10      PAY_3    Fractional           1.0                            False\n",
       "11      PAY_4    Fractional           1.0                            False\n",
       "12      PAY_5    Fractional           1.0                            False\n",
       "13      PAY_6    Fractional           1.0                            False\n",
       "14  BILL_AMT2    Fractional           1.0                            False\n",
       "15  BILL_AMT3    Fractional           1.0                            False\n",
       "16  BILL_AMT4    Fractional           1.0                            False\n",
       "17  BILL_AMT5    Fractional           1.0                            False\n",
       "18  BILL_AMT6    Fractional           1.0                            False\n",
       "19   PAY_AMT2    Fractional           1.0                             True\n",
       "20   PAY_AMT3    Fractional           1.0                             True\n",
       "21   PAY_AMT4    Fractional           1.0                             True\n",
       "22   PAY_AMT5    Fractional           1.0                             True\n",
       "23   PAY_AMT6    Fractional           1.0                             True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61bab8f6-242d-42ee-8928-c7dc1458d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "INFO:sagemaker:Creating monitoring schedule name Built-train-deploy-model-monitor-schedule-2025-03-11-12-26-13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Monitoring Schedule with name: Built-train-deploy-model-monitor-schedule-2025-03-11-12-26-13\n"
     ]
    }
   ],
   "source": [
    "#  Run the following code to set up the frequency for endpoint monitoring.\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(rawbucket,reports_prefix)\n",
    "print(s3_report_path)\n",
    "\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'Built-train-deploy-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa1e338e-e00c-4ebb-9ed7-78f822e18d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>0.201175</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17399.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>18453.0</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>19288.0</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>0.199594</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19347.0</td>\n",
       "      <td>18600.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.284364</td>\n",
       "      <td>0.185736</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.040569</td>\n",
       "      <td>0.289360</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99998.0</td>\n",
       "      <td>16138.0</td>\n",
       "      <td>17758.0</td>\n",
       "      <td>18774.0</td>\n",
       "      <td>20272.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>0.186502</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>6469.0</td>\n",
       "      <td>5138.0</td>\n",
       "      <td>7810.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>7833.0</td>\n",
       "      <td>7130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  PAY_AMT1  BILL_AMT1  LIMIT_BAL  SEX  EDUCATION  MARRIAGE   AGE  \\\n",
       "0      0 -0.341476   0.201175    20000.0  1.0        1.0       2.0  33.0   \n",
       "1      0 -0.136859   0.199594    20000.0  2.0        2.0       2.0  35.0   \n",
       "2      0 -0.284364   0.185736   230000.0  2.0        1.0       1.0  44.0   \n",
       "3      0 -0.040569   0.289360   100000.0  1.0        2.0       1.0  42.0   \n",
       "4      0  0.079132   0.186502   150000.0  1.0        1.0       2.0  29.0   \n",
       "\n",
       "   PAY_0  PAY_2  ...  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  \\\n",
       "0    1.0    2.0  ...    17399.0    19057.0    18453.0    19755.0    19288.0   \n",
       "1    0.0    0.0  ...    19347.0    18600.0    19000.0    19000.0    20000.0   \n",
       "2    1.0   -1.0  ...      949.0     2864.0      933.0        0.0        0.0   \n",
       "3    0.0    0.0  ...    99998.0    16138.0    17758.0    18774.0    20272.0   \n",
       "4   -2.0   -2.0  ...     6917.0      831.0     6469.0     5138.0     7810.0   \n",
       "\n",
       "   PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "0    2260.0       0.0    1600.0       0.0     644.0  \n",
       "1       0.0    1000.0       0.0    1000.0       0.0  \n",
       "2    2873.0     933.0       0.0       0.0       0.0  \n",
       "3    2000.0    2000.0    2000.0    2000.0    2000.0  \n",
       "4     833.0    6488.0    5153.0    7833.0    7130.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test SageMaker Model Monitor performance\n",
    "\n",
    "COLS = data.columns\n",
    "test_full = pd.read_csv('test_data.csv', names = ['Label'] +['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    ")\n",
    "test_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c81a1b8e-15f1-4be4-9032-784f39caab15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n",
       "       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default payment next month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8598bb9-7b33-4e1f-93be-e15d5dc777cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "faketestdata = test_full\n",
    "faketestdata['EDUCATION'] = -faketestdata['EDUCATION'].astype(float)\n",
    "faketestdata['BILL_AMT2']= (faketestdata['BILL_AMT2']//10).astype(float)\n",
    "faketestdata['AGE']= (faketestdata['AGE']-10).astype(float)\n",
    "\n",
    "faketestdata.head()\n",
    "faketestdata.drop(columns=['Label']).to_csv('test-data-input-cols.csv', index = None, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "886e76b1-7f1a-4359-86b3-921748421b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (invoke_endpoint_forever):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_398/1524756942.py\", line 18, in invoke_endpoint_forever\n",
      "  File \"/tmp/ipykernel_398/1524756942.py\", line 11, in invoke_endpoint\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/botocore/client.py\", line 569, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/botocore/client.py\", line 980, in _make_api_call\n",
      "    request_dict = self._convert_to_request_dict(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/botocore/client.py\", line 1047, in _convert_to_request_dict\n",
      "    request_dict = self._serializer.serialize_to_request(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/botocore/validate.py\", line 381, in serialize_to_request\n",
      "    raise ParamValidationError(report=report.generate_report())\n",
      "botocore.exceptions.ParamValidationError: Parameter validation failed:\n",
      "Invalid type for parameter EndpointName, value: {'EndpointArn': 'arn:aws:sagemaker:ap-southeast-1:438465157691:endpoint/cc-training-job-1741689296', 'ResponseMetadata': {'RequestId': '1a738def-ea1e-406c-9ce4-1ad5ad9576b2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1a738def-ea1e-406c-9ce4-1ad5ad9576b2', 'content-type': 'application/x-amz-json-1.1', 'content-length': '99', 'date': 'Tue, 11 Mar 2025 10:50:38 GMT'}, 'RetryAttempts': 0}}, type: <class 'dict'>, valid types: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Run the following code to repeatedly invoke the endpoint with this modified dataset.\n",
    "from threading import Thread\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                          ContentType='text/csv', \n",
    "                                          Body=payload)\n",
    "            time.sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint, 'test-data-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "# Note that you need to stop the kernel to stop the invocations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42aa181a-643c-428f-b0e1-d15ef1a46668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m desc_schedule_result \u001b[38;5;241m=\u001b[39m my_default_monitor\u001b[38;5;241m.\u001b[39mdescribe_schedule()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSchedule status: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(desc_schedule_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonitoringScheduleStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mCopy\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Copy' is not defined"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441bdc2-b9c7-4140-8047-aaf39d72bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\"We created ahourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(600)\n",
    "    mon_executions = my_default_monitor.list_executions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a796b05-bcc8-4e35-868f-293e0d385db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1] # latest execution's index is -1, second to last is -2 and so on..\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()['ProcessingJobStatus']))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if (latest_job['ProcessingJobStatus'] != 'Completed'):\n",
    "        print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c90497-0439-4bb4-b15d-b99444ee44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_uri=latest_execution.output.destination\n",
    "print('Report Uri: {}'.format(report_uri))\n",
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report bucket: {}'.format(report_bucket))\n",
    "print('Report key: {}'.format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2e04e7e-6b44-4703-b919-18d8097b3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: Built-train-deploy-model-monitor-schedule-2025-03-11-12-26-13\n"
     ]
    }
   ],
   "source": [
    "# Step 10 Clean up\n",
    "\n",
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(10) # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9776c52-adc1-443b-969b-2081423273dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2a4c7c37-9492-4ab1-a204-225086b60869',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2a4c7c37-9492-4ab1-a204-225086b60869',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Tue, 11 Mar 2025 12:43:28 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e0ffa8f-4d99-4e8f-a942-7973a6c783f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/data/rawdata.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 rm --recursive s3://sagemaker-ap-southeast-1-438465157691/sagemaker-modelmonitor/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351bc73-bc94-419a-be4e-204c306a1ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
